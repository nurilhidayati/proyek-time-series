# -*- coding: utf-8 -*-
"""Proyek_Time_Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_WRQVd7QIJauZi2uRTFv76zGcEOS_jxO

Nama: Nuril Hidayati

Nomor Registrasi: 1494037162100-621

Pelatihan FGA DTS - Alur Machine Learning Developer

Sidoarjo, Jawa Timur

Dataset : https://www.kaggle.com/muthuj7/weather-dataset
"""

import pandas as pd
import numpy as np
from keras.layers import Dense, LSTM, Bidirectional, Dropout, Lambda
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler

df = pd.read_csv('weatherHistory.csv', parse_dates=['Formatted Date'])
df.head()

df['Formatted Date'] = pd.to_datetime(df['Formatted Date'], utc=True)

df.info()

"""**Data** **Cleaning**"""

df.isna().sum()

df.shape

batas_mae = (df['Wind Speed (km/h)'].max() - df['Wind Speed (km/h)'].min()) * 0.1
batas_mae

"""**Mengambil 20.000 data dan membaginya untuk training dan validation**"""

df_train = df[1:16001]
df_val = df[16002:20002]

plt.figure(figsize=(15,5))
plt.plot(df_train.index, df_train[['Wind Speed (km/h)']])
plt.title('Wind Speed Average',
          fontsize=20);
plt.plot(df_val.index, df_val[['Wind Speed (km/h)']])
plt.title('Wind Speed Average',
          fontsize=20);

minmaxscaler = MinMaxScaler(feature_range = (0,1))
scale_train = minmaxscaler.fit_transform(df_train[['Wind Speed (km/h)']])
df_train[['Wind Speed (km/h)']] = scale_train

scale_val = minmaxscaler.fit_transform(df_val[['Wind Speed (km/h)']])
df_val[['Wind Speed (km/h)']] = scale_val

date_train = df_train['Formatted Date']
wind_train  = df_train['Wind Speed (km/h)'].values

date_val = df_val['Formatted Date']
wind_val = df_val['Wind Speed (km/h)'].values

date_train.shape

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

"""**Modelling**"""

train_set = windowed_dataset(wind_train, window_size=60, batch_size=100, shuffle_buffer=1000)
test_set  = windowed_dataset(wind_val, window_size=60, batch_size=100, shuffle_buffer=1000)
model = tf.keras.models.Sequential([
                                    tf.keras.layers.LSTM(64, return_sequences=True),
                                    tf.keras.layers.LSTM(64),
                                    tf.keras.layers.Dense(30, activation='relu'),
                                    tf.keras.layers.Dropout(0.2),
                                    tf.keras.layers.Dense(30, activation='relu'),
                                    tf.keras.layers.Dropout(0.5),
                                    tf.keras.layers.Dense(1)
])

"""**Membuat Class Callback**"""

from tensorflow.keras.callbacks import Callback
class myCallback(Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('mae')<0.09 and logs.get('val_mae')<0.09):
            print("MAE has reached below 10%")
            self.model.stop_training = True
callbacks = myCallback()

"""**Melatih Model**"""

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set,
                    epochs=200,
                    callbacks=[callbacks],
                    validation_data=test_set)

model.evaluate(test_set)

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

plt.plot(history.history['mae'])
plt.title('Model accuracy')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()